## <center>ğŸè´ªåƒè›‡æ¸¸æˆåŠåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¨¡å‹ğŸ¤–</center>

#### English version README included. 

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=flat&logo=PyTorch&logoColor=white)](https://pytorch.org/)

> **ä» 0 åˆ° 22000 åˆ†çš„è¿›åŒ–ï¼** æœ¬é¡¹ç›®å±•ç¤ºäº†ä¸€ä¸ªåŸºäº DQN çš„æ™ºèƒ½ä½“å¦‚ä½•é€šè¿‡è¿ç§»å­¦ä¹ ï¼Œä»æŒæ¡åŸºç¡€ç”Ÿå­˜åˆ°ç©è½¬å¤æ‚çš„ Combo & Frenzy æœºåˆ¶ã€‚ä½ å¯ä»¥è‡ªå·±ä¸Šæ‰‹æ¸¸ç©ï¼Œä¹Ÿå¯ä»¥è®­ç»ƒæˆ–è§‚çœ‹Agentæ¸¸ç©


## âš¡ Quick Start

### 1ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ Play the Game

```bash
python eating_snake.py
```

### 3ï¸âƒ£ Watch Trained AI Play

```bash
python agent_test.py
```


---

## ğŸš€ æ ¸å¿ƒäº®ç‚¹ (Key Highlights)

### 1. è¿ç§»å­¦ä¹ ç­–ç•¥ (Transfer Learning Strategy)
è¿™æ˜¯æœ¬é¡¹ç›®æˆåŠŸçš„å…³é”®ã€‚ç”±äºå¸¦æœ‰ Combo æœºåˆ¶çš„ç¯å¢ƒå¥–åŠ±å‡½æ•°æå…¶å¤æ‚ï¼Œç›´æ¥è®­ç»ƒå®¹æ˜“å¯¼è‡´æ¨¡å‹é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚
* **é˜¶æ®µä¸€ï¼šåŸºç¡€è¯¾** - åœ¨ç»å…¸è´ªåƒè›‡ç¯å¢ƒä¸‹è®­ç»ƒ 1000 å±€ï¼Œå»ºç«‹åŸºç¡€é¿éšœå’Œå¯»è·¯é€»è¾‘ã€‚
* **é˜¶æ®µäºŒï¼šä¸“ä¸šè¯¾** - è¿ç§»æƒé‡è‡³â€œåŠ å¼ºç‰ˆâ€ç¯å¢ƒã€‚AI å¸¦ç€â€œå…ˆéªŒçŸ¥è¯†â€è¿›åœºï¼Œä»…ç”¨ 100 å±€å³å®ç°åˆ†æ•°çˆ†å‘ã€‚

### 2. ç²¾ç‚¼çš„ 11 ç»´çŠ¶æ€ç‰¹å¾ (11-D State Representation)
æ”¾å¼ƒäº†æ²‰é‡çš„åƒç´ è¾“å…¥ï¼Œé‡‡ç”¨äº†é«˜æ•ˆçš„ 11 ç»´å‘é‡ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨ CPU ä¸Šä¹Ÿèƒ½æé€Ÿæ”¶æ•›ï¼š
- **å±é™©æ¢æµ‹** (3ç»´)ï¼šå‰æ–¹ã€å·¦ä¾§ã€å³ä¾§æ˜¯å¦æœ‰éšœç¢ã€‚
- **ç§»åŠ¨æ–¹å‘** (4ç»´)ï¼šå½“å‰å‰è¿›æ–¹å‘çš„ One-hot ç¼–ç ã€‚
- **é£Ÿç‰©å®šä½** (4ç»´)ï¼šé£Ÿç‰©ç›¸å¯¹äºè›‡å¤´çš„ä¸Šä¸‹å·¦å³ä½ç½®ã€‚

### 3. è‡ªå®šä¹‰æ¸¸æˆæœºåˆ¶ (Advanced Mechanics)
ä¸åŒäºä¼ ç»Ÿè´ªåƒè›‡ï¼Œæœ¬é¡¹ç›®åœ¨ Pygame ç¯å¢ƒä¸­å¼•å…¥äº†ï¼š
- **Combo ç³»ç»Ÿ**ï¼šçŸ­æ—¶é—´å†…è¿ç»­åƒè±†åˆ†æ•°æˆå€å¢é•¿ã€‚
- **Frenzy Mode**ï¼šè§¦å‘ç‹‚æš´æ¨¡å¼ï¼Œè·å¾—è¶…é«˜é¢å¾—åˆ†å¥–åŠ±ã€‚

### 4. å¼€ç®±å³ç© (Play Now!)
ä½ å¯ä»¥è‡ªå·±ä¸Šæ‰‹æ¸¸ç©ï¼Œä¹Ÿå¯ä»¥ç›´æ¥ç”¨ä¸è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæ¸¸ç©ã€‚


---

## ğŸ“Š è®­ç»ƒè¡¨ç° (Performance)

* **æœ€é«˜å¾—åˆ† (Max Score):** `22000+`
* **æ”¶æ•›é€Ÿåº¦:** è¿ç§»å­¦ä¹ åï¼Œæ¨¡å‹è¿…é€Ÿé€‚åº”ï¼Œèƒ½å¤Ÿç¨³å®šçš„åˆ°è¾¾1000+åˆ†
* **ç¨³å®šæ€§:** åœ¨ $Epsilon = 0.05$ çš„æ¢ç´¢ç‡ä¸‹ï¼Œä¾ç„¶èƒ½ä¿æŒæé«˜çš„é«˜åˆ†è§¦å‘é¢‘ç‡ã€‚



---

## ğŸ“‚ æ–‡ä»¶ç»“æ„ (Project Structure)

```text
â”œâ”€â”€ agent_test.py # æµ‹è¯•è´ªåƒè›‡Agent       
â”œâ”€â”€ agent_training.py # è®­ç»ƒè´ªåƒè›‡Agent
â”œâ”€â”€ classic_agent_test.py # æµ‹è¯•ç»å…¸è´ªåƒè›‡Agent
â”œâ”€â”€ classic_agent_training.py # è®­ç»ƒç»å…¸è´ªåƒè›‡Agent
â”œâ”€â”€ eating_snake.py # å¯ä»¥è‡ªå·±ä¸Šæ‰‹ç©çš„è´ªåƒè›‡
â”œâ”€â”€ requirements.txt  # ç¯å¢ƒä¾èµ–
â”œâ”€â”€ snake_agent.pth # è´ªåƒè›‡Agentæ¨¡å‹
â”œâ”€â”€ snake_memory.pkl # è´ªåƒè›‡Agentè®°å¿†
â”œâ”€â”€ classic_snake_agent.pth # ç»å…¸è´ªåƒè›‡Agentæ¨¡å‹
â””â”€â”€ classic_snake_memory.pkl # ç»å…¸è´ªåƒè›‡Agentè®°å¿†
```

***


## <center>ğŸ Snake Game with Reinforcement Learning Agent ğŸ¤–</center>

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=flat&logo=PyTorch&logoColor=white)](https://pytorch.org/)

> **From 0 to 22,000+ score evolution!**  
> This project demonstrates how a DQN-based agent learns progressively through transfer learning â€” from basic survival skills to mastering complex Combo & Frenzy mechanics.  
> You can play the game yourself, train the agent, or watch the trained AI in action.

---

## ğŸš€ Key Highlights

### 1. Transfer Learning Strategy

This is the core reason behind the success of the project.  
Due to the highly complex reward structure introduced by the Combo system, direct training often leads to local optima.

* **Stage 1: Fundamentals**  
  Train the agent for 1000 episodes in the classic Snake environment to learn collision avoidance and pathfinding.

* **Stage 2: Advanced Training**  
  Transfer the learned weights into the enhanced environment.  
  With prior knowledge, the agent achieves score explosion within only 100 episodes.

---

### 2. Efficient 11-D State Representation

Instead of computationally expensive pixel inputs, we use an efficient 11-dimensional feature vector, allowing fast convergence even on CPU:

- **Danger Detection (3 dims)**  
  Whether there is danger straight, left, or right.

- **Movement Direction (4 dims)**  
  One-hot encoding of the current movement direction.

- **Food Location (4 dims)**  
  Relative position of the food (up, down, left, right).

---

### 3. Advanced Game Mechanics

Unlike traditional Snake, this project introduces several custom mechanics implemented in Pygame:

- **Combo System**  
  Consecutive food consumption within a short time window multiplies rewards.

- **Frenzy Mode**  
  Special state with significantly boosted scoring potential.

---

### 4. Play Now!

You can:

- ğŸ® Play manually
- ğŸ¤– Watch the trained AI play
- ğŸ§  Train the agent yourself

---

## ğŸ“Š Performance

- **Max Score:** `22000+`
- **Convergence Speed:** After transfer learning, the agent rapidly adapts and consistently reaches 1000+ scores.
- **Stability:** Even with an exploration rate of $Epsilon = 0.05$, the agent maintains a high frequency of high-score runs.

---

## ğŸ“‚ Project Structure

```text
â”œâ”€â”€ agent_test.py                 # Test enhanced Snake agent
â”œâ”€â”€ agent_training.py             # Train enhanced Snake agent
â”œâ”€â”€ classic_agent_test.py         # Test classic Snake agent
â”œâ”€â”€ classic_agent_training.py     # Train classic Snake agent
â”œâ”€â”€ eating_snake.py               # Playable Snake game
â”œâ”€â”€ requirements.txt              # Dependencies
â”œâ”€â”€ snake_agent.pth               # Trained enhanced model
â”œâ”€â”€ snake_memory.pkl              # Replay memory
â”œâ”€â”€ classic_snake_agent.pth       # Classic model
â””â”€â”€ classic_snake_memory.pkl      # Classic replay memory


```
